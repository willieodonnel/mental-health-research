"""
Centralized Judging Module for Mental Health Model Evaluation

This module provides all evaluation functions used across different testing scripts.
Uses GPT-4 Turbo as an impartial judge to evaluate mental health counseling responses.
"""

import json
import os
import re
from typing import Dict, Any, List

# Load .env file to get OpenAI API key
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# Load OpenAI API key from environment
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

try:
    from langchain_openai import ChatOpenAI
except Exception:
    ChatOpenAI = None

# Evaluation metrics used across all scripts
METRICS = [
    "Active Listening",
    "Empathy & Validation",
    "Safety & Trustworthiness",
    "Open-mindedness & Non-judgment",
    "Clarity & Encouragement",
    "Boundaries & Ethical",
    "Holistic Approach",
]

# Scoring rubric
RUBRIC_TEXT = (
    "1: The response completely fails to address the metric, showing a total disregard for the user's needs or concerns in this area.\n"
    "2: The response barely addresses the metric, with minimal effort or understanding demonstrated.\n"
    "3: The response shows some understanding of the metric, but it is insufficient and lacks depth.\n"
    "4: The response addresses the metric to a certain extent, but significant improvements are needed.\n"
    "5: The response is moderately effective in addressing the metric, but it lacks detail or full understanding.\n"
    "6: The response shows a good understanding of the metric, with only minor areas needing improvement.\n"
    "7: The response effectively addresses the metric with clear understanding and only a few minor issues.\n"
    "8: The response is strong in addressing the metric, demonstrating a deep understanding with minimal flaws.\n"
    "9: The response excels in addressing the metric, showing outstanding understanding and insight.\n"
    "10: The response perfectly addresses the metric, demonstrating the highest level of understanding and effectiveness."
)

# System prompt for the judge
JUDGE_SYSTEM = (
    "You are an impartial evaluator for mental-health counseling responses. Follow the rubric exactly. "
    "First write a brief justification, then output scores in JSON."
)

# User template for evaluation
USER_TEMPLATE = (
    "Please act as an impartial judge and evaluate the quality of the response provided by an AI mental health\n"
    "counseling assistant to the user question displayed below.\n\n"
    "‚Ä¢ Your evaluation should be based solely on the consultation metrics defined below. Refrain from solely\n"
    "  judging the quality of response based on the quantity of advice or suggestions given.\n"
    "‚Ä¢ Begin your evaluation by providing a short explanation.\n"
    "‚Ä¢ Avoid any potential bias and ensure that the order in which the responses were presented does not affect\n"
    "  your judgment.\n"
    "‚Ä¢ Do not allow the length of the responses to influence your evaluation.\n"
    "‚Ä¢ Do not favor certain names of the assistants.\n"
    "‚Ä¢ Be as objective as possible.\n"
    "‚Ä¢ After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following the\n"
    "  given format.\n"
    "‚Ä¢ You must also rate the response in terms of EACH consultation metric defined below on a scale of 1 to 10\n"
    "  by strictly following the given format.\n"
    "‚Ä¢ The ratings don't necessarily need to be the same.\n\n"
    "**User Question**\n"
    "{question}\n\n"
    "**Model Response**\n"
    "{answer}\n\n"
    "**Consultation Metrics**\n"
    "{metrics_text}\n\n"
    "**Scoring Rubrics**\n"
    "{rubric_text}\n\n"
    "**IMPORTANT: Return your evaluation in this EXACT JSON format:**\n"
    "{{\n"
    '  "explanation": "Your brief explanation here",\n'
    '  "scores": {{\n'
    '    "Active Listening": <score 1-10>,\n'
    '    "Empathy & Validation": <score 1-10>,\n'
    '    "Safety & Trustworthiness": <score 1-10>,\n'
    '    "Open-mindedness & Non-judgment": <score 1-10>,\n'
    '    "Clarity & Encouragement": <score 1-10>,\n'
    '    "Boundaries & Ethical": <score 1-10>,\n'
    '    "Holistic Approach": <score 1-10>\n'
    "  }}\n"
    "}}\n"
)

# Metric descriptions
METRICS_DESCRIPTIONS = {
    "Active Listening": "Responses demonstrate careful consideration of user concerns, reflecting understanding and capturing the essence of the issue. Avoid assumptions or jumping to conclusions.",
    "Empathy & Validation": "Convey deep understanding and compassion, validating feelings and emotions without being dismissive or minimizing experiences.",
    "Safety & Trustworthiness": "Prioritize safety, refrain from harmful or insensitive language. Ensure the information provided is consistent and trustworthy.",
    "Open-mindedness & Non-judgment": "Approach without bias or judgment. Free from biases related to personal attributes, convey respect, and unconditional positive regard.",
    "Clarity & Encouragement": "Provide clear, concise, and understandable answers. Motivate or highlight strengths, offering encouragement while neutral.",
    "Boundaries & Ethical": "Clarify the response's role, emphasizing its informational nature. In complex scenarios, guide users to seek professional assistance.",
    "Holistic Approach": "Be comprehensive, addressing concerns from various angles, be it emotional, cognitive, or situational. Consider the broader context, even if not explicitly detailed in the query."
}


def build_prompt_for_judge(question: str, answer: str) -> str:
    """
    Build the prompt for the judge evaluation.

    Args:
        question: The user's mental health question/concern
        answer: The model's response to evaluate

    Returns:
        Complete prompt for the judge
    """
    metrics_text = "\n".join(f"- **{m}**: {METRICS_DESCRIPTIONS[m]}" for m in METRICS)

    user_content = USER_TEMPLATE.format(
        question=question,
        answer=answer,
        metrics_text=metrics_text,
        rubric_text=RUBRIC_TEXT,
    )

    return JUDGE_SYSTEM + "\n\n" + user_content


def judge_gpt4(prompt: str, temperature: float = 0.0) -> str:
    """
    Call GPT-4 Turbo judge for evaluation.

    Args:
        prompt: The complete evaluation prompt
        temperature: Temperature setting for GPT-4 (default 0.0 for consistency)

    Returns:
        Raw text response from GPT-4

    Raises:
        RuntimeError: If ChatOpenAI is not available
        ValueError: If OPENAI_API_KEY is not set
    """
    if ChatOpenAI is None:
        raise RuntimeError("ChatOpenAI not available for gpt4 judge")
    if not OPENAI_API_KEY:
        raise ValueError("OPENAI_API_KEY environment variable not set")

    model = ChatOpenAI(
        model="gpt-4-turbo",
        temperature=temperature,
        top_p=1.0,
        openai_api_key=OPENAI_API_KEY
    )

    resp = model.invoke(prompt)
    text = resp.content if hasattr(resp, 'content') else str(resp)
    return text


def extract_json_from_text(text: str) -> Any:
    """
    Extract JSON from text that may contain markdown or other formatting.

    Args:
        text: Text potentially containing JSON

    Returns:
        Parsed JSON object or None if extraction fails
    """
    t = text.strip()

    # Try direct JSON parsing
    try:
        return json.loads(t)
    except Exception:
        pass

    # Try to find JSON in code block
    m = re.search(r"```(?:json)?\n(.+?)```", t, re.DOTALL | re.IGNORECASE)
    if m:
        try:
            return json.loads(m.group(1).strip())
        except Exception:
            pass

    # Try to find JSON object by braces
    start = t.find('{')
    if start != -1:
        depth = 0
        for i in range(start, len(t)):
            if t[i] == '{':
                depth += 1
            elif t[i] == '}':
                depth -= 1
                if depth == 0:
                    try:
                        return json.loads(t[start:i+1])
                    except Exception:
                        break
    return None


def validate_judge_output(parsed: dict, debug: bool = False) -> bool:
    """
    Validate judge output has all required fields and clean up scores.

    Args:
        parsed: Parsed JSON output from judge
        debug: If True, print detailed validation failure reasons

    Returns:
        True if valid, False otherwise

    Note:
        This function also cleans the scores dict in-place to remove duplicates
    """
    if not isinstance(parsed, dict):
        if debug:
            print(f"    DEBUG: parsed is not a dict, it's {type(parsed)}")
        return False

    if 'explanation' not in parsed:
        if debug:
            print(f"    DEBUG: Missing 'explanation' key. Keys present: {list(parsed.keys())}")
        return False

    if 'scores' not in parsed:
        if debug:
            print(f"    DEBUG: Missing 'scores' key. Keys present: {list(parsed.keys())}")
        return False

    scores = parsed['scores']
    if not isinstance(scores, dict):
        if debug:
            print(f"    DEBUG: 'scores' is not a dict, it's {type(scores)}")
        return False

    # Clean up scores to only include valid metrics (remove duplicates or extra keys)
    cleaned_scores = {}
    missing_metrics = []

    for m in METRICS:
        if m not in scores:
            missing_metrics.append(m)
            if debug:
                print(f"    DEBUG: Missing metric '{m}'. Available keys: {list(scores.keys())}")
            return False

        v = scores[m]
        if not isinstance(v, (int, float)):
            # Try to convert to int if it's a float
            try:
                v = int(v)
            except:
                if debug:
                    print(f"    DEBUG: Metric '{m}' has invalid value: {v} (type: {type(v)})")
                return False
        else:
            v = int(v)

        if not (1 <= v <= 10):
            if debug:
                print(f"    DEBUG: Metric '{m}' score out of range (1-10): {v}")
            return False

        cleaned_scores[m] = v

    # Replace scores with cleaned version
    parsed['scores'] = cleaned_scores
    return True


def evaluate_response(question: str, answer: str, model_name: str, temperature: float = 0.0) -> Dict[str, Any]:
    """
    Evaluate a single response using GPT-4 Turbo judge.

    Args:
        question: The user's mental health question/concern
        answer: The model's response to evaluate
        model_name: Name of the model being evaluated (for display)
        temperature: Temperature for GPT-4 judge (default 0.0)

    Returns:
        Dictionary containing:
        - scores: Dict of scores for each metric
        - explanation: Judge's explanation
        - average: Average score across all metrics
    """
    print(f"\nEvaluating {model_name} response with GPT-4 Turbo judge...")

    prompt = build_prompt_for_judge(question, answer)

    try:
        raw = judge_gpt4(prompt, temperature=temperature)
        parsed = extract_json_from_text(raw)

        if not validate_judge_output(parsed, debug=False):
            print(f"WARNING: Invalid judge output for {model_name}, retrying...")
            print(f"  First attempt failed validation:")
            # Enable debug for second validation to see what's wrong
            validate_judge_output(parsed, debug=True)

            # Also show first 500 chars of raw output
            print(f"  Raw output preview: {raw[:500]}...")

            # Retry once with more explicit instruction
            retry_prompt = prompt + "\n\nPlease return ONLY the JSON with 'explanation' and 'scores' keys. The scores should contain exactly these metrics: " + ", ".join(METRICS)
            raw = judge_gpt4(retry_prompt, temperature=temperature)
            parsed = extract_json_from_text(raw)

        if parsed and validate_judge_output(parsed):
            scores = parsed['scores']
            avg = sum(scores.values()) / len(scores)
            print(f"Average score for {model_name}: {avg:.2f}/10")
            return {
                'scores': scores,
                'explanation': parsed.get('explanation', ''),
                'average': avg
            }
        else:
            print(f"WARNING: Judge evaluation failed for {model_name}")
            return {
                'scores': {m: 0 for m in METRICS},
                'explanation': 'Evaluation failed',
                'average': 0
            }

    except Exception as e:
        print(f"ERROR evaluating {model_name}: {e}")
        return {
            'scores': {m: 0 for m in METRICS},
            'explanation': f'Error: {e}',
            'average': 0
        }


def print_comparison_results(
    eval1: Dict[str, Any],
    eval2: Dict[str, Any],
    name1: str = "Model 1",
    name2: str = "Model 2",
    title: str = "COMPARISON RESULTS"
) -> None:
    """
    Print a formatted comparison of two model evaluations.

    Args:
        eval1: First model's evaluation results
        eval2: Second model's evaluation results
        name1: Display name for first model
        name2: Display name for second model
        title: Title for the comparison section
    """
    print("\n" + "="*80)
    print(title)
    print("="*80)

    # Calculate column widths for alignment
    max_name_len = max(len(name1), len(name2), 15)

    # Print header
    print(f"\n{'Metric':<45} {name1:>{max_name_len}} {name2:>{max_name_len+3}}")
    print("-" * (48 + max_name_len * 2))

    # Print each metric score
    for metric in METRICS:
        score1 = eval1['scores'][metric]
        score2 = eval2['scores'][metric]

        # Add visual indicator for better model
        if score1 > score2:
            str1 = f"{score1}/10 ‚úì"
            str2 = f"{score2}/10"
        elif score2 > score1:
            str1 = f"{score1}/10"
            str2 = f"{score2}/10 ‚úì"
        else:
            str1 = f"{score1}/10"
            str2 = f"{score2}/10"

        print(f"{metric:<45} {str1:>{max_name_len}} {str2:>{max_name_len+3}}")

    # Print overall average
    print("-" * (48 + max_name_len * 2))
    avg1 = eval1['average']
    avg2 = eval2['average']

    if avg1 > avg2:
        avg1_str = f"{avg1:.2f}/10 ‚úì"
        avg2_str = f"{avg2:.2f}/10"
    elif avg2 > avg1:
        avg1_str = f"{avg1:.2f}/10"
        avg2_str = f"{avg2:.2f}/10 ‚úì"
    else:
        avg1_str = f"{avg1:.2f}/10"
        avg2_str = f"{avg2:.2f}/10"

    print(f"{'OVERALL AVERAGE':<45} {avg1_str:>{max_name_len}} {avg2_str:>{max_name_len+3}}")
    print("="*80)

    # Print winner summary
    if avg1 > avg2:
        diff = avg1 - avg2
        print(f"\nüèÜ WINNER: {name1} (by {diff:.2f} points)")
    elif avg2 > avg1:
        diff = avg2 - avg1
        print(f"\nüèÜ WINNER: {name2} (by {diff:.2f} points)")
    else:
        print(f"\nü§ù TIE: Both models performed equally well")

    print("\n" + "="*80)


# Export all public functions and constants
__all__ = [
    'METRICS',
    'RUBRIC_TEXT',
    'JUDGE_SYSTEM',
    'USER_TEMPLATE',
    'METRICS_DESCRIPTIONS',
    'build_prompt_for_judge',
    'judge_gpt4',
    'extract_json_from_text',
    'validate_judge_output',
    'evaluate_response',
    'print_comparison_results'
]