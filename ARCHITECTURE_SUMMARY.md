# Architecture Summary - Mental Health Pipeline

## Quick Reference

### Inference (Response Generation)
- **Model**: Mistral-7B-Instruct-v0.2
- **Where**: RTX 5080 GPU (local)
- **API Key**: ❌ NOT needed
- **Cost**: Free after setup
- **Privacy**: 100% local

### Evaluation (Quality Judging)
- **Model**: GPT-4
- **Where**: OpenAI API (cloud)
- **API Key**: ✅ Required (optional feature)
- **Cost**: ~$0.01-0.03 per evaluation
- **Privacy**: Responses sent to OpenAI for scoring

---

## IMPORTANT: What Uses What

### ❌ ChatGPT/GPT-4 is NOT used for inference

The pipeline does **NOT** use ChatGPT or GPT-4 to generate mental health counseling responses.

### ✅ Mistral-7B-Instruct is used for ALL inference

All mental health responses are generated by **Mistral-7B-Instruct** running locally on your RTX 5080.

### ✅ GPT-4 is ONLY used as an optional judge

GPT-4 is **ONLY** used to evaluate and score responses (if you choose to run evaluations). It does NOT generate any counseling responses.

---

## File Guide

### Main Inference Files (USE THESE)

| File | Purpose | Model | API Key? |
|------|---------|-------|----------|
| [mental_health_inference.py](mental_health_inference.py) | **Main inference script** | Mistral-7B | ❌ No |
| [run_custom_questions.py](run_custom_questions.py) | Batch custom questions | Mistral-7B | ❌ No |
| [evaluation_local.py](evaluation_local.py) | Evaluate with GPT-4 judge | Mistral-7B (inference) + GPT-4 (judge) | ✅ Yes (for judge) |

### Legacy Files (GPT-4 Pipeline - Not Recommended)

| File | Purpose | Model | API Key? |
|------|---------|-------|----------|
| [pipeline.py](pipeline.py) | Legacy 3-stage pipeline | GPT-4 (all stages) | ✅ Yes |
| [main.py](main.py) | Legacy demo script | GPT-4 (all stages) | ✅ Yes |
| [evaluation.py](evaluation.py) | Legacy evaluation | GPT-4 (inference + judge) | ✅ Yes |

---

## Architecture Diagrams

### Primary Method: Mistral-7B Local Inference

```
┌─────────────┐
│ User Input  │
└──────┬──────┘
       │
       ▼
┌──────────────────────────────────┐
│  Mistral-7B-Instruct (RTX 5080) │
│  - PyTorch Nightly (CUDA 12.8)  │
│  - FP16 or 4-bit quantization   │
│  - NO API calls                  │
│  - 100% local                    │
└──────┬───────────────────────────┘
       │
       ▼
┌──────────────────────────┐
│ Mental Health Response   │
│ (Empathetic, Professional)│
└──────────────────────────┘
```

### Optional: Evaluation with GPT-4 Judge

```
┌─────────────┐
│ User Input  │
└──────┬──────┘
       │
       ▼
┌──────────────────────────┐
│  Mistral-7B-Instruct    │
│  (Generates Response)    │
└──────┬───────────────────┘
       │
       ▼
┌──────────────────────────┐
│  Mental Health Response  │
└──────┬───────────────────┘
       │
       ▼
┌──────────────────────────┐
│  GPT-4 Judge (Optional)  │
│  - Scores response 1-10  │
│  - 7 quality metrics     │
│  - ONLY for evaluation   │
└──────┬───────────────────┘
       │
       ▼
┌──────────────────────────┐
│  Evaluation Results      │
│  (JSONL + CSV)          │
└──────────────────────────┘
```

### Legacy: Three-Stage GPT-4 Pipeline (Not Primary Method)

```
┌─────────────┐
│ User Input  │
└──────┬──────┘
       │
       ▼
┌──────────────────────────┐
│  GPT-4 Stage 1           │
│  (Clinical Transform)    │
└──────┬───────────────────┘
       │
       ▼
┌──────────────────────────┐
│  memory.txt              │
└──────┬───────────────────┘
       │
       ▼
┌──────────────────────────┐
│  GPT-4 Stage 2           │
│  (Professional Response) │
└──────┬───────────────────┘
       │
       ▼
┌──────────────────────────┐
│  GPT-4 Stage 3           │
│  (Compassionate Tone)    │
└──────┬───────────────────┘
       │
       ▼
┌──────────────────────────┐
│  Final Response          │
└──────────────────────────┘
```

---

## Quick Commands

### Run Inference (No API Key)

```bash
# Quick demo with 3 example questions
python mental_health_inference.py

# Process custom questions
python run_custom_questions.py

# Interactive chat mode
python -c "from mental_health_inference import MentalHealthInferencePipeline; MentalHealthInferencePipeline().interactive_mode()"
```

### Run Evaluation (Requires API Key)

```bash
# Setup (one time)
echo "OPENAI_API_KEY=your-key-here" > .env

# Generate test questions
python evaluation_local.py --generate_test_set 20

# Evaluate (Mistral generates, GPT-4 judges)
python evaluation_local.py --questions test_set_20.jsonl --num_samples 20
```

---

## System Prompt (Mistral-7B)

The Mistral-7B model uses this system prompt for all inferences:

```
You are a helpful and empathetic mental health counseling assistant.
Please answer the mental health questions based on the user's description.
The assistant gives helpful, comprehensive, and appropriate answers to
the user's questions.
```

This single prompt incorporates counseling best practices and guides the model to generate:
- Empathetic responses
- Evidence-based guidance
- Comprehensive answers
- Active listening and validation

---

## Performance Metrics (RTX 5080)

### Mistral-7B-Instruct Local Inference

| Metric | Value |
|--------|-------|
| Load Time (first run) | 30-60 seconds |
| Load Time (cached) | 5-10 seconds |
| Generation Speed | 40-60 tokens/second |
| Short Response (256 tokens) | 4-6 seconds |
| Medium Response (512 tokens) | 8-12 seconds |
| VRAM Usage (4-bit) | ~3.5 GB |
| VRAM Usage (FP16) | ~14 GB |
| Quality Score | 7-8/10 |
| Cost per Response | Free |

### GPT-4 Evaluation (Optional)

| Metric | Value |
|--------|-------|
| Evaluation Time | 5-10 seconds per question |
| Cost per Evaluation | ~$0.01-0.03 |
| Metrics Evaluated | 7 (MentalChat16K standard) |

---

## FAQ

### Q: Do I need an OpenAI API key?

**A**: No, not for inference. You only need an API key if you want to run evaluations with GPT-4 as a judge.

### Q: Does this use ChatGPT to generate responses?

**A**: No. All responses are generated by Mistral-7B-Instruct running locally on your RTX 5080.

### Q: What is GPT-4 used for?

**A**: GPT-4 is ONLY used as an optional judge to evaluate and score the quality of responses generated by Mistral-7B. It does NOT generate counseling responses.

### Q: Can I run this without internet?

**A**: Yes! Mistral-7B inference runs 100% locally. You only need internet for:
1. Initial model download (one-time, ~4GB)
2. Optional evaluation with GPT-4 judge

### Q: Is my data private?

**A**: Yes! All inference happens locally on your GPU. Your questions and responses never leave your computer. If you run evaluations, only the responses (not questions) are sent to OpenAI for scoring.

### Q: Why not use GPT-4 for inference?

**A**: This project focuses on local, private, cost-free inference. Mistral-7B provides good quality mental health responses while maintaining privacy and avoiding API costs.

### Q: Can I switch to GPT-4 for better quality?

**A**: Yes, the legacy three-stage GPT-4 pipeline is available in `pipeline.py`, but it requires an API key and incurs costs. The main method is Mistral-7B local inference.

---

## Model Comparison

| Aspect | Mistral-7B (Local) | GPT-4 (Cloud) |
|--------|-------------------|---------------|
| **Quality** | Good (7-8/10) | Excellent (9/10) |
| **Speed** | 8-12 seconds | 5-10 seconds |
| **Cost** | Free | $0.01-0.03/response |
| **Privacy** | 100% local | Sent to OpenAI |
| **Internet** | Not required | Required |
| **API Key** | Not needed | Required |
| **VRAM** | 3.5-14 GB | N/A |
| **Setup** | Model download | API key only |

---

## Conclusion

**Primary Method**: Mistral-7B-Instruct on RTX 5080
- ✅ Local inference (no ChatGPT/GPT-4)
- ✅ No API key needed
- ✅ 100% private
- ✅ Cost-free after setup

**Optional Evaluation**: GPT-4 as Judge
- ✅ Scores response quality
- ✅ Does NOT generate responses
- ⚠️ Requires API key

**Legacy Method**: GPT-4 Three-Stage Pipeline
- ⚠️ Not the primary method
- ⚠️ Requires API key and incurs costs
- ℹ️ Available for research comparison

For questions, see [COMPREHENSIVE_GUIDE.md](COMPREHENSIVE_GUIDE.md) or [RTX_5080_SETUP.md](RTX_5080_SETUP.md).
